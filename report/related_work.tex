\section{Related Work}
\label{s:related_work}

\noindent {\bf Data visualization for model interpretability.} Approaches that rely on data visualization to increase model interpretability include DrawNet / NetDissect~\cite{netdissect_2017}, which help quantify interpretability of models by measuring and visualizing similarity between known human-interpretable concepts and decisions taken by the CNN model. Another recent example of visualization-based approach is SmoothGrad~\cite{smoothgrad_2017}, which helps recover learned concepts by removing noise from activation images via sequential applications of Gaussian noise.  We believe the latter is complementary to our work, in that our summaries could benefit from the Gaussian filter based denoising step proposed in SmoothGrad, but leave that as future work.

\noindent {\bf Model compression.} In ``Deep Compression'' \cite{deepCompression_2015} the authors introduce Huffman coding compression and model weight quantization to CNN models with the goal of reducing model sizes without loss in model accuracy.  While their work does not share the same goal of model interpretability as in our project, their approach is related to ours in that they show a model's accuracy does not change significantly despite agressive changes to its weights (quantization of learned weights), and to its structure (e.g., pruning of ``unimportant'' connections).  This corroborates with our hypothesis that it is possible to understand what decisions a CNN is taking even if most information on it is summarized.